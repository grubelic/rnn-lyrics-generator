{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3243,
     "status": "ok",
     "timestamp": 1641206463891,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "Ptn4yR3RF9Gl"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Dropout,Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import string, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import utils as np_utils\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(songs, artist, title=None):\n",
    "    if title is None: return songs[(songs.artist == artist)]\n",
    "    return songs[(songs.artist == artist) & (songs.title == title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('./datasets/labeled_lyrics_cleaned.csv',\n",
    "                    usecols=[\"artist\", \"seq\", \"song\"])\n",
    "songs.rename(columns={\"seq\": \"lyrics\", \"song\": \"title\"}, inplace=True)\n",
    "songs.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't worry about a thing\r\n",
      "'Cause every little thing gonna be alright\r\n",
      "Singing' don't worry about a thing\r\n",
      "'Cause every little thing gonna be alright\r\n",
      "\r\n",
      "Rise up this mornin'\r\n",
      "Smiled with the risin' sun\r\n",
      "Three little birds\r\n",
      "Pitch by my doorstep\r\n",
      "Singin' sweet songs\r\n",
      "Of melodies pure and true\r\n",
      "Saying', (this is my message to you)\r\n",
      "\r\n",
      "Singing' don't worry 'bout a thing\r\n",
      "'Cause every little thing gonna be alright\r\n",
      "Singing' don't worry (don't worry) 'bout a thing\r\n",
      "'Cause every little thing gonna be alright\r\n",
      "\r\n",
      "Rise up this mornin'\r\n",
      "Smiled with the risin' sun\r\n",
      "Three little birds\r\n",
      "Pitch by my doorstep\r\n",
      "Singin' sweet songs\r\n",
      "Of melodies pure and true\r\n",
      "Sayin', this is my message to you\r\n",
      "\r\n",
      "Singin' don't worry about a thing, worry about a thing, oh\r\n",
      "Every little thing gonna be alright, don't worry\r\n",
      "Singin' don't worry about a thing, I won't worry\r\n",
      "\"'Cause every little thing gonna be alright\r\n",
      "\r\n",
      "Singin' don't worry about a thing\r\n",
      "'Cause every little thing gonna be alright, I won't worry\r\n",
      "Singin', don't worry about a thing\r\n",
      "'Cause every little thing gonna be alright\r\n",
      "Singin' don't worry about a thing, oh no\r\n",
      "'Cause every little thing gonna be alright\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "# Bob Marley, Bon Jovi, Boney M., Eminem, Iron Maiden, Madonna,\n",
    "# R.E.M., Red Hot Chilli Peppers, The Beatles, The Rolling Stones, U2\n",
    "# for name, his_songs in songs.groupby(\"artist\"):\n",
    "#     if len(his_songs) > 95:\n",
    "#         print(name, len(his_songs))\n",
    "# print('\\n'.join(sorted(set(songs.artist))))\n",
    "# del name, his_songs\n",
    "print(next(iter(get_lyrics(songs, \"Bob Marley\", \"Three Little Birds\").lyrics)))\n",
    "print(len(list(get_lyrics(songs, \"Bob Marley\").lyrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1641206464632,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "XcCf1TJlsvxu",
    "outputId": "b69e1f77-83e2-4e36-e055-dc26837d6f74"
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_lyrics = list(set(get_lyrics(songs, \"Bob Marley\").lyrics))\n",
    "all_lyrics = list(set(songs.lyrics))[0:2000]\n",
    "lyrics_cnt = len(all_lyrics)\n",
    "'''\n",
    "all_lyrics = list(set(songs.lyrics))[0:1000]\n",
    "\n",
    "all_lyrics = []\n",
    "all_lyrics.extend(list(lyrics.values))\n",
    "#print(all_lyrics)\n",
    "\n",
    "for a in all_lyrics:\n",
    "  if not isinstance(a, str):\n",
    "    print(\"found one\")\n",
    "    all_lyrics.remove(a)\n",
    "\n",
    "print(len(all_lyrics))\n",
    "# all_lyrics = all_lyrics[:200]\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "all_lyrics = all_lyrics[0:1000]\n",
    "'''\n",
    "None\n",
    "# poetry = pd.read_csv('/content/gdrive/My Drive/Kaggle/PoetryFoundationData.csv',quotechar='\"')\n",
    "\n",
    "# poetry.head()\n",
    "\n",
    "# poem = poetry.Poem\n",
    "# poem_cnt = len(poem)\n",
    "\n",
    "# all_poems = []\n",
    "# all_poems.extend(list(poem.values))\n",
    "\n",
    "\n",
    "# for a in all_poems:\n",
    "#   if not isinstance(a, str):\n",
    "#     all_poems.remove(a)\n",
    "\n",
    "# all_poems = all_poems[:50]\n",
    "# joined = all_poems+all_lyrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████                                                                         | 154/2000 [00:06<01:01, 29.84it/s]"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "\n",
    "all_lyrics_en = []\n",
    "for song in tqdm(all_lyrics):\n",
    "    _cnt_y = 0\n",
    "    _cnt_n = 0\n",
    "    for line in song.split('\\r\\n'):\n",
    "        for word in line.split():\n",
    "            if d.check(word):\n",
    "                _cnt_y += 1\n",
    "            else:\n",
    "                _cnt_n += 1\n",
    "    if _cnt_n > (_cnt_y + _cnt_n) * 0.4:\n",
    "        # print(song)\n",
    "        continue\n",
    "    all_lyrics_en.append(song)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = all_lyrics_en[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1641206464633,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "ZDT_rYS1ECrs"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(txt):\n",
    "    # txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    # txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    rgx = re.compile(r\"\\[[^\\n\\]]*]\")\n",
    "    return rgx.sub('', txt) \n",
    "\n",
    "def get_sequence_of_tokens(corpus,\n",
    "                           lines_in_n_gram=2, \n",
    "                           endl_as_token=False,\n",
    "                           ignoring_empty_lines=False):\n",
    "    if endl_as_token:\n",
    "        corpus.append([\"endl\"])\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus.pop()\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for song in tqdm(corpus):\n",
    "        if ignoring_empty_lines:\n",
    "            rgx = re.compile(r\"(\\r\\n)+\")\n",
    "            song = rgx.sub(\"\\r\\n\", song)\n",
    "        if endl_as_token:\n",
    "            song = song.replace(\"\\r\\n\", \" endl\\r\\n\")\n",
    "        # print(song.split('\\r\\n'))\n",
    "        token_lists = tokenizer.texts_to_sequences(song.split('\\r\\n'))\n",
    "        # print(token_lists)\n",
    "        for i in range(lines_in_n_gram, len(token_lists)):\n",
    "            flat = [jt for it in token_lists[i-lines_in_n_gram:i] for jt in it]\n",
    "            for j in range(1, len(flat)):\n",
    "                n_gram_sequence = flat[:j+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        '''\n",
    "        prev_token_list = []\n",
    "        for line in song.split('\\r\\n'):\n",
    "            if not line:\n",
    "                continue\n",
    "#             print(line)\n",
    "            token_list = list(tokenizer.texts_to_sequences([line])[0])\n",
    "            #print(token_list)\n",
    "            # input_sequences.append(list(prev_token_list) + list(token_list))\n",
    "            two_lines = prev_token_list + token_list\n",
    "            for i in range(1, len(two_lines)):\n",
    "                n_gram_sequence = two_lines[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "#             print(input_sequences)\n",
    "            prev_token_list = token_list\n",
    "        '''\n",
    "    return input_sequences, total_words\n",
    "\n",
    "def get_padded_sequences(input_sequences):\n",
    "    max_seq_len = max( [len(x) for x in input_sequences])\n",
    "    padded_input_sequences =  np.array(pad_sequences(input_sequences, \n",
    "                                                     maxlen=max_seq_len, padding='pre'))\n",
    "    return padded_input_sequences, max_seq_len\n",
    "\n",
    "\n",
    "def prep_train_data(padded_input_sequences,total_words):\n",
    "    \n",
    "    x_train = padded_input_sequences[:,:-1]\n",
    "    labels = padded_input_sequences[:,-1]\n",
    "    labels = keras.utils.np_utils.to_categorical(labels, num_classes=total_words)\n",
    "    \n",
    "    return x_train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1641206466641,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "VyLnUPGMDUsw"
   },
   "outputs": [],
   "source": [
    "# corpus = [clean_text(x) for x in all_lyrics]\n",
    "corpus = all_lyrics\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r')\n",
    "input_sequences, total_words = get_sequence_of_tokens(corpus, 4, True, False)\n",
    "padded_input_sequences, max_seq_len = get_padded_sequences(input_sequences)\n",
    "x_train, labels = prep_train_data(padded_input_sequences,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(sorted(tokenizer.word_index)), end=\"\\n\\n\\n\")\n",
    "print('\\n'.join(tokenizer.sequences_to_texts(input_sequences[:20])))\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4482,
     "status": "ok",
     "timestamp": 1641206471117,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "mxF0iwqmIifs",
    "outputId": "2555338e-cd4b-446d-ceb9-854d3612592b"
   },
   "outputs": [],
   "source": [
    "def lstm_model(max_seq_len,total_words):\n",
    "    \n",
    "    input_len = max_seq_len - 1 #zadnju predvidam\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add an LSTM Layer\n",
    "    model.add(Bidirectional(CuDNNLSTM(150, return_sequences=True)))  # A dropout layer for regularisation\n",
    "    model.add(Dropout(0.2))# Add another LSTM Layer\n",
    "    model.add(CuDNNLSTM(100))\n",
    "    model.add(Dense(total_words/2, activation='relu'))\n",
    "    # In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "    #(# Pick a loss function and an optimizer)print(model.summary())\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = lstm_model(max_seq_len,total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 14066,
     "status": "error",
     "timestamp": 1641206485165,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "W_IxA7C-GsvA",
    "outputId": "a7d680f2-9528-424f-f632-8ed2fd46e2a6"
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, labels, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1641206485156,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "WeweULSNKrHc"
   },
   "outputs": [],
   "source": [
    "def generate_lyrics(seed_txt, next_words_cnt , max_seq_len, model):\n",
    "  \n",
    "    for i in range(0, next_words_cnt):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_txt])[0]\n",
    "        padded_token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        \n",
    "        prediction = np.argmax(model.predict(padded_token_list), axis=-1)\n",
    "\n",
    "        #print('prediction',prediction)\n",
    "        #tokenizer.word_index.items() --> rijecnik (rijec,index)\n",
    "        \n",
    "        \n",
    "        for (word, index) in tokenizer.word_index.items():\n",
    "            output_word = \"\"\n",
    "            if (prediction == index):\n",
    "                seed_txt += \" \" + word\n",
    "                break\n",
    "      \n",
    "        \n",
    "    return seed_txt.title()\n",
    "\n",
    "def generate_lyrics_endl_as_token(seed_txt,\n",
    "                                  lines_in_n_gram,\n",
    "                                  num_lines,\n",
    "                                  max_line_len,\n",
    "                                  max_seq_len,\n",
    "                                  model):\n",
    "        st_list = seed_txt.replace('\\r', \"\").split('\\n')\n",
    "        st_list = st_list[max(0, len(st_list) - lines_in_n_gram):]\n",
    "        token_list = tokenizer.texts_to_sequences(st_list)\n",
    "        lines_remaining = num_lines - len(token_list)\n",
    "        flat = [jt for it in token_list for jt in it]\n",
    "        endl_token = tokenizer.texts_to_sequences([\"endl\"])[0][0]\n",
    "        tokenized_song = []\n",
    "        curr_line = []\n",
    "        while lines_remaining:\n",
    "            # print(lines_remaining)\n",
    "            if len(curr_line) >= max_line_len:\n",
    "                prediction = endl_token\n",
    "            else:\n",
    "                padded_token_list = pad_sequences([flat], maxlen=max_seq_len-1, padding='pre')\n",
    "                prediction = np.argmax(model.predict(padded_token_list), axis=-1)[0]\n",
    "            curr_line.append(prediction)\n",
    "            flat.append(prediction)\n",
    "            if prediction == endl_token:\n",
    "                lines_remaining -= 1\n",
    "                if len(token_list) >= lines_in_n_gram: del token_list[0]\n",
    "                token_list.append(curr_line)\n",
    "                tokenized_song.append(curr_line[:-1])\n",
    "                curr_line = []\n",
    "                flat = [jt for it in token_list for jt in it]\n",
    "        # print(tokenized_song[1:])\n",
    "        print('\\n'.join(\n",
    "            tokenizer.sequences_to_texts(tokenized_song)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1641206485159,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "Z6luPyeqRMrn"
   },
   "outputs": [],
   "source": [
    "# model.save(\"model_200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1641206485161,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "clRleXG5Ktgl"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "unique = []\n",
    "for i in range(0,len(corpus)):\n",
    "    unique.append(corpus[i].split())\n",
    "\n",
    "'''\n",
    "for i in range(0,20):\n",
    "  random_list = unique[random.randint(0,len(unique)-1)]\n",
    "  seed = random_list[random.randint(0,len(random_list))]\n",
    "  print( generate_lyrics(seed,random.randint(3,15),max_seq_len, model ) )\n",
    "'''\n",
    "# duljina pjesme iz iste distribucije kao ostale\n",
    "# koristenje corpusa tu?\n",
    "num_lines = random.choice([it.count(\"\\r\\n\") for it in all_lyrics])\n",
    "max_line_len = max([max([len(it.split()) for it in song.split(\"\\r\\n\")]) for song in all_lyrics])\n",
    "\n",
    "\n",
    "seed_txt = \"\"\"Here's a little song I wrote\n",
    "You might want to sing it note for note\n",
    "Don't worry, be happy\"\"\"\n",
    "\n",
    "generate_lyrics_endl_as_token(seed_txt,\n",
    "                              lines_in_n_gram=4,\n",
    "                              num_lines=num_lines,\n",
    "                              max_line_len=max_line_len,\n",
    "                              max_seq_len=max_seq_len,\n",
    "                              model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1641206485163,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "0v9-8EZrX-zp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lyrics_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
