{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3243,
     "status": "ok",
     "timestamp": 1641206463891,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "Ptn4yR3RF9Gl"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, CuDNNLSTM, LSTM, Dense, SimpleRNN, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import string, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import utils as np_utils\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(songs, artist, title=None):\n",
    "    if title is None: return songs[(songs.artist == artist)]\n",
    "    return songs[(songs.artist == artist) & (songs.title == title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('./datasets/labeled_lyrics_cleaned.csv',\n",
    "                    usecols=[\"artist\", \"seq\", \"song\"])\n",
    "songs.rename(columns={\"seq\": \"lyrics\", \"song\": \"title\"}, inplace=True)\n",
    "songs.drop_duplicates(inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob Marley, Bon Jovi, Boney M., Eminem, Iron Maiden, Madonna,\n",
    "# R.E.M., Red Hot Chilli Peppers, The Beatles, The Rolling Stones, U2\n",
    "# for name, his_songs in songs.groupby(\"artist\"):\n",
    "#     if len(his_songs) > 95:\n",
    "#         print(name, len(his_songs))\n",
    "# print('\\n'.join(sorted(set(songs.artist))))\n",
    "# del name, his_songs\n",
    "# print(next(iter(get_lyrics(songs, \"Bob Marley\", \"Three Little Birds\").lyrics)))\n",
    "# print(len(list(get_lyrics(songs, \"Bob Marley\").lyrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1641206464632,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "XcCf1TJlsvxu",
    "outputId": "b69e1f77-83e2-4e36-e055-dc26837d6f74"
   },
   "outputs": [],
   "source": [
    "\n",
    "# all_lyrics = list(set(get_lyrics(songs, \"Bob Marley\").lyrics))\n",
    "all_lyrics = list(set(songs.lyrics))[0:2000]\n",
    "lyrics_cnt = len(all_lyrics)\n",
    "'''\n",
    "all_lyrics = list(set(songs.lyrics))[0:1000]\n",
    "\n",
    "all_lyrics = []\n",
    "all_lyrics.extend(list(lyrics.values))\n",
    "#print(all_lyrics)\n",
    "\n",
    "for a in all_lyrics:\n",
    "  if not isinstance(a, str):\n",
    "    print(\"found one\")\n",
    "    all_lyrics.remove(a)\n",
    "\n",
    "print(len(all_lyrics))\n",
    "# all_lyrics = all_lyrics[:200]\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "all_lyrics = all_lyrics[0:1000]\n",
    "'''\n",
    "None\n",
    "# poetry = pd.read_csv('/content/gdrive/My Drive/Kaggle/PoetryFoundationData.csv',quotechar='\"')\n",
    "\n",
    "# poetry.head()\n",
    "\n",
    "# poem = poetry.Poem\n",
    "# poem_cnt = len(poem)\n",
    "\n",
    "# all_poems = []\n",
    "# all_poems.extend(list(poem.values))\n",
    "\n",
    "\n",
    "# for a in all_poems:\n",
    "#   if not isinstance(a, str):\n",
    "#     all_poems.remove(a)\n",
    "\n",
    "# all_poems = all_poems[:50]\n",
    "# joined = all_poems+all_lyrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:43<00:00, 45.88it/s]\n"
     ]
    }
   ],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "\n",
    "all_lyrics_en = []\n",
    "for song in tqdm(all_lyrics):\n",
    "    _cnt_y = 0\n",
    "    _cnt_n = 0\n",
    "    for line in song.split('\\r\\n'):\n",
    "        for word in line.split():\n",
    "            if d.check(word):\n",
    "                _cnt_y += 1\n",
    "            else:\n",
    "                _cnt_n += 1\n",
    "    if _cnt_n > (_cnt_y + _cnt_n) * 0.4:\n",
    "        # print(song)\n",
    "        continue\n",
    "    all_lyrics_en.append(song)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lyrics = all_lyrics_en[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1641206464633,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "ZDT_rYS1ECrs"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(txt):\n",
    "    # txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    # txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    rgx = re.compile(r\"\\[[^\\n\\]]*]\")\n",
    "    return rgx.sub('', txt) \n",
    "\n",
    "def get_sequence_of_tokens(corpus,\n",
    "                           lines_in_n_gram=2, \n",
    "                           endl_as_token=False,\n",
    "                           ignoring_empty_lines=False):\n",
    "    if endl_as_token:\n",
    "        corpus.append([\"endl\"])\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    corpus.pop()\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    input_sequences = []\n",
    "    for song in tqdm(corpus):\n",
    "        if ignoring_empty_lines:\n",
    "            rgx = re.compile(r\"(\\r\\n)+\")\n",
    "            song = rgx.sub(\"\\r\\n\", song)\n",
    "        if endl_as_token:\n",
    "            song = song.replace(\"\\r\\n\", \" endl\\r\\n\")\n",
    "        # print(song.split('\\r\\n'))\n",
    "        token_lists = tokenizer.texts_to_sequences(song.split('\\r\\n'))\n",
    "        # print(token_lists)\n",
    "        for i in range(lines_in_n_gram, len(token_lists)):\n",
    "            flat = [jt for it in token_lists[i-lines_in_n_gram:i] for jt in it]\n",
    "            for j in range(1, len(flat)):\n",
    "                n_gram_sequence = flat[:j+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        '''\n",
    "        prev_token_list = []\n",
    "        for line in song.split('\\r\\n'):\n",
    "            if not line:\n",
    "                continue\n",
    "#             print(line)\n",
    "            token_list = list(tokenizer.texts_to_sequences([line])[0])\n",
    "            #print(token_list)\n",
    "            # input_sequences.append(list(prev_token_list) + list(token_list))\n",
    "            two_lines = prev_token_list + token_list\n",
    "            for i in range(1, len(two_lines)):\n",
    "                n_gram_sequence = two_lines[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "#             print(input_sequences)\n",
    "            prev_token_list = token_list\n",
    "        '''\n",
    "    return input_sequences, total_words\n",
    "\n",
    "def get_padded_sequences(input_sequences):\n",
    "    max_seq_len = max( [len(x) for x in input_sequences])\n",
    "    padded_input_sequences =  np.array(pad_sequences(input_sequences, \n",
    "                                                     maxlen=max_seq_len, padding='pre'))\n",
    "    return padded_input_sequences, max_seq_len\n",
    "\n",
    "\n",
    "def prep_train_data(padded_input_sequences,total_words):\n",
    "    \n",
    "    x_train = padded_input_sequences[:,:-1]\n",
    "    labels = padded_input_sequences[:,-1]\n",
    "    labels = keras.utils.np_utils.to_categorical(labels, num_classes=total_words)\n",
    "    \n",
    "    return x_train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1641206466641,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "VyLnUPGMDUsw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1422.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# corpus = [clean_text(x) for x in all_lyrics]\n",
    "corpus = all_lyrics\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r')\n",
    "input_sequences, total_words = get_sequence_of_tokens(corpus, 4, True, False)\n",
    "padded_input_sequences, max_seq_len = get_padded_sequences(input_sequences)\n",
    "x_train, labels = prep_train_data(padded_input_sequences,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "# print('\\n'.join(sorted(tokenizer.word_index)), end=\"\\n\\n\\n\")\n",
    "# print('\\n'.join(tokenizer.sequences_to_texts(input_sequences[:20])))\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4482,
     "status": "ok",
     "timestamp": 1641206471117,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "mxF0iwqmIifs",
    "outputId": "2555338e-cd4b-446d-ceb9-854d3612592b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 64, 10)            40410     \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 128)               17792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4041)              521289    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 579,491\n",
      "Trainable params: 579,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def lstm_model(max_seq_len,total_words):\n",
    "    \n",
    "    input_len = max_seq_len - 1 #zadnju predvidam\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add an LSTM Layer\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True)))  # A dropout layer for regularisation\n",
    "    model.add(Dropout(0.2))# Add another LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words/2, activation='relu'))\n",
    "    # In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "    #(# Pick a loss function and an optimizer)print(model.summary())\n",
    "    \n",
    "    return model\n",
    "'''\n",
    "def lstm_model(max_seq_len,total_words):\n",
    "    \n",
    "    input_len = max_seq_len - 1 #zadnju predvidam\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    # model.add(CuDNNLSTM(128))\n",
    "    # model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(SimpleRNN(128))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "model = lstm_model(max_seq_len,total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 14066,
     "status": "error",
     "timestamp": 1641206485165,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "W_IxA7C-GsvA",
    "outputId": "a7d680f2-9528-424f-f632-8ed2fd46e2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "4002/4002 [==============================] - 445s 111ms/step - loss: 1.2403\n",
      "Epoch 22/50\n",
      "4002/4002 [==============================] - 446s 111ms/step - loss: 1.2180\n",
      "Epoch 23/50\n",
      "4002/4002 [==============================] - 446s 111ms/step - loss: 1.1961\n",
      "Epoch 24/50\n",
      "4002/4002 [==============================] - 448s 112ms/step - loss: 1.1746\n",
      "Epoch 25/50\n",
      "4002/4002 [==============================] - 450s 112ms/step - loss: 1.1536\n",
      "Epoch 26/50\n",
      "4002/4002 [==============================] - 450s 112ms/step - loss: 1.1428\n",
      "Epoch 27/50\n",
      "4002/4002 [==============================] - 449s 112ms/step - loss: 1.1197\n",
      "Epoch 28/50\n",
      "4002/4002 [==============================] - 450s 113ms/step - loss: 1.1088\n",
      "Epoch 29/50\n",
      "4002/4002 [==============================] - 451s 113ms/step - loss: 1.0906\n",
      "Epoch 30/50\n",
      "4002/4002 [==============================] - 450s 112ms/step - loss: 1.0818\n",
      "Epoch 31/50\n",
      "4002/4002 [==============================] - 451s 113ms/step - loss: 1.0700\n",
      "Epoch 32/50\n",
      "4002/4002 [==============================] - 449s 112ms/step - loss: 1.0531\n",
      "Epoch 33/50\n",
      "4002/4002 [==============================] - 449s 112ms/step - loss: 1.0471\n",
      "Epoch 34/50\n",
      "4002/4002 [==============================] - 449s 112ms/step - loss: 1.0352\n",
      "Epoch 35/50\n",
      "4002/4002 [==============================] - 450s 112ms/step - loss: 1.0231\n",
      "Epoch 36/50\n",
      "4002/4002 [==============================] - 451s 113ms/step - loss: 1.0175\n",
      "Epoch 37/50\n",
      "4002/4002 [==============================] - 459s 115ms/step - loss: 1.0050\n",
      "Epoch 38/50\n",
      "4002/4002 [==============================] - 455s 114ms/step - loss: 0.9960\n",
      "Epoch 39/50\n",
      "4002/4002 [==============================] - 451s 113ms/step - loss: 0.9884\n",
      "Epoch 40/50\n",
      "4002/4002 [==============================] - 449s 112ms/step - loss: 0.9791\n",
      "Epoch 41/50\n",
      "4002/4002 [==============================] - 450s 112ms/step - loss: 0.9766\n",
      "Epoch 42/50\n",
      "4002/4002 [==============================] - 450s 112ms/step - loss: 0.9639\n",
      "Epoch 43/50\n",
      "4002/4002 [==============================] - 454s 113ms/step - loss: 0.9632\n",
      "Epoch 44/50\n",
      "4002/4002 [==============================] - 450s 113ms/step - loss: 0.9575\n",
      "Epoch 45/50\n",
      "4002/4002 [==============================] - 444s 111ms/step - loss: 0.9498\n",
      "Epoch 46/50\n",
      "4002/4002 [==============================] - 448s 112ms/step - loss: 0.9398\n",
      "Epoch 47/50\n",
      "4002/4002 [==============================] - 449s 112ms/step - loss: 0.9359\n",
      "Epoch 48/50\n",
      "4002/4002 [==============================] - 453s 113ms/step - loss: 0.9335\n",
      "Epoch 49/50\n",
      "4002/4002 [==============================] - 457s 114ms/step - loss: 0.9306\n",
      "Epoch 50/50\n",
      "4002/4002 [==============================] - 457s 114ms/step - loss: 0.9235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20bc8e7dbe0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, labels, epochs=50, verbose=1, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1641206485156,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "WeweULSNKrHc"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_lyrics(seed_txt, next_words_cnt , max_seq_len, model):\n",
    "  \n",
    "    for i in range(0, next_words_cnt):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_txt])[0]\n",
    "        padded_token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        \n",
    "        prediction = np.argmax(model.predict(padded_token_list), axis=-1)\n",
    "\n",
    "        #print('prediction',prediction)\n",
    "        #tokenizer.word_index.items() --> rijecnik (rijec,index)\n",
    "        \n",
    "        \n",
    "        for (word, index) in tokenizer.word_index.items():\n",
    "            output_word = \"\"\n",
    "            if (prediction == index):\n",
    "                seed_txt += \" \" + word\n",
    "                break\n",
    "      \n",
    "        \n",
    "    return seed_txt.title()\n",
    "\n",
    "def generate_lyrics_endl_as_token(seed_txt,\n",
    "                                  lines_in_n_gram,\n",
    "                                  num_lines,\n",
    "                                  max_line_len,\n",
    "                                  max_seq_len,\n",
    "                                  model,\n",
    "                                  no_anaphora=False):\n",
    "        st_list = seed_txt.replace('\\r', \"\").replace('\\n', \" endl\\n\").split('\\n')\n",
    "        if not(st_list[-1].endswith(\" endl\")):\n",
    "            st_list[-1] = st_list[-1] + \" endl\"\n",
    "        st_list = st_list[max(0, len(st_list) - lines_in_n_gram):]\n",
    "        token_list = tokenizer.texts_to_sequences(st_list)\n",
    "        lines_remaining = num_lines - len(token_list)\n",
    "        flat = [jt for it in token_list for jt in it]\n",
    "        endl_token = tokenizer.texts_to_sequences([\"endl\"])[0][0]\n",
    "        tokenized_song = []\n",
    "        curr_line = []\n",
    "        while lines_remaining:\n",
    "            # print(lines_remaining)\n",
    "            if len(curr_line) >= max_line_len:\n",
    "                prediction = endl_token\n",
    "            else:\n",
    "                padded_token_list = pad_sequences([flat], maxlen=max_seq_len-1, padding='pre')\n",
    "                predictions = model.predict(padded_token_list)\n",
    "                if (no_anaphora):\n",
    "                    for prev_line in token_list:\n",
    "                        if (len(curr_line) < len(prev_line)\n",
    "                            and curr_line == prev_line[:len(curr_line)]):\n",
    "                            # print(curr_line)\n",
    "                            word_in_prev_line = prev_line[len(curr_line)]\n",
    "                            # print(word_in_prev_line)\n",
    "                            predictions[0][word_in_prev_line] = np.min(predictions, axis=-1)\n",
    "                prediction = np.argmax(predictions, axis=-1)[0]\n",
    "            curr_line.append(prediction)\n",
    "            flat.append(prediction)\n",
    "            if prediction == endl_token:\n",
    "                lines_remaining -= 1\n",
    "                if len(token_list) >= lines_in_n_gram: del token_list[0]\n",
    "                token_list.append(curr_line)\n",
    "                tokenized_song.append(curr_line[:-1])\n",
    "                curr_line = []\n",
    "                flat = [jt for it in token_list for jt in it]\n",
    "        # print(tokenized_song[1:])\n",
    "        print('\\n'.join(\n",
    "            tokenizer.sequences_to_texts(tokenized_song)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1641206485159,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "Z6luPyeqRMrn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_200_50\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model_200_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1641206485161,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "clRleXG5Ktgl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so i said\n",
      "i don't had a calculator\n",
      "to know i'm gonna see you later\n",
      "and i don't need a secretary\n",
      "for the next door all make it a rose to my mind\n",
      "you get new more of sleep\n",
      "a futile attempt to uphold your claim\n",
      "as i don't even myself but i want to do so many not believe some sleep\n",
      "\n",
      "this fire's down they put mind by the good move\n",
      "your heart i was the day\n",
      "when i know what i saw\n",
      "but i don't care how i was mine\n",
      "i wish that each reasons\n",
      "on the salt lakeline\n",
      "casey jones\n",
      "got another poppa\n",
      "my special time\n",
      "\n",
      "why would you ever get\n",
      "it was\n",
      "so old are love yes you push you weak\n",
      "i'd keep it in my list\n",
      "and if there's what the way was mine\n",
      "oh you can hear\n",
      "\n",
      "slow to me for the only way oh\n",
      "to the harp and his back\n",
      "but you've almost gonna before to fly\n",
      "all i need you listen\n",
      "we shall come on a him\n",
      "his child was mine\n",
      "the looked is on in the mouth\n",
      "make a blanket with my baby\n",
      "through a good little dream\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''unique = []\n",
    "for i in range(0,len(corpus)):\n",
    "    unique.append(corpus[i].split())'''\n",
    "\n",
    "'''\n",
    "for i in range(0,20):\n",
    "  random_list = unique[random.randint(0,len(unique)-1)]\n",
    "  seed = random_list[random.randint(0,len(random_list))]\n",
    "  print( generate_lyrics(seed,random.randint(3,15),max_seq_len, model ) )\n",
    "'''\n",
    "# duljina pjesme iz iste distribucije kao ostale\n",
    "# koristenje corpusa tu?\n",
    "num_lines = random.choice([it.count(\"\\r\\n\") for it in all_lyrics])\n",
    "max_line_len = max([max([len(it.split()) for it in song.split(\"\\r\\n\")]) for song in all_lyrics])\n",
    "\n",
    "\n",
    "seed_txt = \"\"\"Here's a little song I wrote\n",
    "You might want to sing it note for note\n",
    "Don't worry, be happy\"\"\"\n",
    "\n",
    "# seed_txt = \"\"\n",
    "\n",
    "generate_lyrics_endl_as_token(seed_txt,\n",
    "                              lines_in_n_gram=4,\n",
    "                              num_lines=num_lines,\n",
    "                              max_line_len=max_line_len,\n",
    "                              max_seq_len=max_seq_len,\n",
    "                              model=model,\n",
    "                              no_anaphora=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1641206485163,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "0v9-8EZrX-zp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lyrics_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
