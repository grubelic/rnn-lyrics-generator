{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3243,
     "status": "ok",
     "timestamp": 1641206463891,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "Ptn4yR3RF9Gl"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, SimpleRNN, Dropout, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import string, os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from keras import utils as np_utils\n",
    "from tqdm import tqdm # pip3 install tqdm\n",
    "import enchant # pip3 install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDL_AS_TOKEN = True # False not supported\n",
    "DATA_SUBSET = \"60%en_US\" # or \"Artist - Bob Dylan\"\n",
    "LYRICS_LIMIT = 200 # 0 for no limit, don't use negative numbers\n",
    "REMOVE_TAGS = True\n",
    "N_GRAMS_LINES = 4\n",
    "DROP_DUPLICATES = True\n",
    "REMOVE_EMPTY_ROWS = False\n",
    "REMOVE_DOUBLE_EMPTY_ROWS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(songs, artist, title=None):\n",
    "    if title is None: return songs[(songs.artist == artist)]\n",
    "    return songs[(songs.artist == artist) & (songs.title == title)]\n",
    "\n",
    "def is_correct_language(lyrics, enchant_dict, retain_rate):\n",
    "    ws = keras.preprocessing.text.text_to_word_sequence(lyrics)\n",
    "    return sum(map(enchant_dict.check, ws)) >= retain_rate * len(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv('./datasets/labeled_lyrics_cleaned.csv',\n",
    "                    usecols=[\"artist\", \"seq\", \"song\"])\n",
    "songs.rename(columns={\"seq\": \"lyrics\", \"song\": \"title\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:05<00:00, 39.25it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "songs.dropna(inplace=True)\n",
    "if REMOVE_TAGS:\n",
    "    songs.lyrics = songs.lyrics.str.strip().replace(re.compile(r\"\\r|(\\[[^\\n\\]]*])\"), '')\n",
    "else:\n",
    "    songs.lyrics = songs.lyrics.str.strip().replace('\\r', '')\n",
    "if REMOVE_EMPTY_ROWS:\n",
    "    songs.lyrics = songs.lyrics.str.replace(re.compile(r\"\\n+\"), '\\n')\n",
    "if REMOVE_DOUBLE_EMPTY_ROWS and not REMOVE_EMPTY_ROWS:\n",
    "    songs.lyrics = songs.lyrics.str.replace(re.compile(r\"\\n\\n+\"), '\\n\\n')\n",
    "if ENDL_AS_TOKEN:\n",
    "    songs.lyrics = songs.lyrics.str.replace('\\n', ' endl\\n') + ' endl'\n",
    "if DROP_DUPLICATES:\n",
    "    songs.drop_duplicates(subset=\"lyrics\", inplace=True, ignore_index=True)\n",
    "all_lyrics = []\n",
    "if '%' in DATA_SUBSET:\n",
    "    song_iterator = songs.iterrows()\n",
    "    language = DATA_SUBSET[DATA_SUBSET.index('%') + 1:]\n",
    "    percentage = int(DATA_SUBSET[:DATA_SUBSET.index('%')]) * 0.01\n",
    "    d = enchant.Dict(language)\n",
    "    for i in tqdm(range(LYRICS_LIMIT or len(songs))):\n",
    "        index, song = next(song_iterator)\n",
    "        while(not(is_correct_language(song.lyrics, d, percentage))):\n",
    "            index, song = next(song_iterator)\n",
    "        all_lyrics.append(song.lyrics)\n",
    "elif DATA_SUBSET.startswith(\"Artist - \"):\n",
    "    artists_songs = get_lyrics(songs, DATA_SUBSET[9:])\n",
    "    if LYRICS_LIMIT:\n",
    "        all_lyrics = list(artists_songs.lyrics.iloc[:LYRICS_LIMIT])\n",
    "    else:\n",
    "        all_lyrics = list(artists_songs.lyrics)\n",
    "lyrics_cnt = len(all_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1788\n",
      "dict_keys(['word_counts', 'word_docs', 'filters', 'split', 'lower', 'num_words', 'document_count', 'char_level', 'oov_token', 'index_docs', 'word_index', 'index_word'])\n"
     ]
    }
   ],
   "source": [
    "# Bob Marley, Bon Jovi, Boney M., Eminem, Iron Maiden, Madonna,\n",
    "# R.E.M., Red Hot Chilli Peppers, The Beatles, The Rolling Stones, U2\n",
    "# for name, his_songs in songs.groupby(\"artist\"):\n",
    "#     if len(his_songs) > 95:\n",
    "#         print(name, len(his_songs))\n",
    "# print('\\n'.join(sorted(set(songs.artist))))\n",
    "# del name, his_songs\n",
    "# print(next(iter(get_lyrics(songs, \"Bob Marley\", \"Three Little Birds\").lyrics)))\n",
    "# print(len(list(get_lyrics(songs, \"Bob Dylan\").lyrics)))\n",
    "# _temp_t = Tokenizer()\n",
    "# _temp_t.fit_on_texts(songs.lyrics)\n",
    "from pprint import pprint\n",
    "pprint(tokenizer.word_index[\"hello\"])\n",
    "pprint(vars(tokenizer).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1641206464633,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "ZDT_rYS1ECrs"
   },
   "outputs": [],
   "source": [
    "def get_sequence_of_tokens(corpus, tokenizer, lines_in_n_gram=2):\n",
    "    input_sequences = []\n",
    "    for song in tqdm(corpus):\n",
    "        token_lists = tokenizer.texts_to_sequences(song.split('\\n'))\n",
    "        for i in range(lines_in_n_gram, len(token_lists)):\n",
    "            flat = [jt for it in token_lists[i-lines_in_n_gram:i] for jt in it]\n",
    "            for j in range(1, len(flat)):\n",
    "                n_gram_sequence = flat[:j+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences\n",
    "\n",
    "def get_padded_sequences(input_sequences):\n",
    "    max_seq_len = max([len(x) for x in input_sequences])\n",
    "    padded_input_sequences = np.array(pad_sequences(input_sequences, \n",
    "                                                     maxlen=max_seq_len,\n",
    "                                                     padding='pre'))\n",
    "    return padded_input_sequences, max_seq_len\n",
    "\n",
    "\n",
    "def prep_train_data(padded_input_sequences, total_words):\n",
    "    \n",
    "    x_train = padded_input_sequences[:,:-1]\n",
    "    labels = padded_input_sequences[:,-1]\n",
    "    labels = keras.utils.np_utils.to_categorical(labels, num_classes=total_words)\n",
    "    \n",
    "    return x_train, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2014,
     "status": "ok",
     "timestamp": 1641206466641,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "VyLnUPGMDUsw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 950.41it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_lyrics)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = get_sequence_of_tokens(all_lyrics, tokenizer, 4)\n",
    "padded_input_sequences, max_seq_len = get_padded_sequences(input_sequences)\n",
    "x_train, labels = prep_train_data(padded_input_sequences,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "# print('\\n'.join(sorted(tokenizer.word_index)), end=\"\\n\\n\\n\")\n",
    "# print('\\n'.join(tokenizer.sequences_to_texts(input_sequences[:20])))\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4482,
     "status": "ok",
     "timestamp": 1641206471117,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "mxF0iwqmIifs",
    "outputId": "2555338e-cd4b-446d-ceb9-854d3612592b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 57, 10)            31580     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               71168     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3158)              407382    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 510,130\n",
      "Trainable params: 510,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "def lstm_model(max_seq_len,total_words):\n",
    "    \n",
    "    input_len = max_seq_len - 1 #zadnju predvidam\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add an LSTM Layer\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True)))  # A dropout layer for regularisation\n",
    "    model.add(Dropout(0.2))# Add another LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words/2, activation='relu'))\n",
    "    # In the last layer, the shape should be equal to the total number of words present in our corpus\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')\n",
    "    #(# Pick a loss function and an optimizer)print(model.summary())\n",
    "    \n",
    "    return model\n",
    "'''\n",
    "def lstm_model(max_seq_len,total_words):\n",
    "    \n",
    "    input_len = max_seq_len - 1 #zadnju predvidam\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1 - LSTM Layer\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.1))\n",
    "    \n",
    "    # model.add(SimpleRNN(128))\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "model = lstm_model(max_seq_len,total_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 14066,
     "status": "error",
     "timestamp": 1641206485165,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "W_IxA7C-GsvA",
    "outputId": "a7d680f2-9528-424f-f632-8ed2fd46e2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5278/5278 [==============================] - 29s 5ms/step - loss: 5.2354\n",
      "Epoch 2/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 4.3359\n",
      "Epoch 3/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 3.6385\n",
      "Epoch 4/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 3.1323\n",
      "Epoch 5/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 2.7718\n",
      "Epoch 6/20\n",
      "5278/5278 [==============================] - 29s 5ms/step - loss: 2.5083\n",
      "Epoch 7/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 2.3100\n",
      "Epoch 8/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 2.1544\n",
      "Epoch 9/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 2.0274\n",
      "Epoch 10/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.9262\n",
      "Epoch 11/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.8388\n",
      "Epoch 12/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.7621\n",
      "Epoch 13/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.6972\n",
      "Epoch 14/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.6427\n",
      "Epoch 15/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.5940\n",
      "Epoch 16/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.5516\n",
      "Epoch 17/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.5128\n",
      "Epoch 18/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.4786\n",
      "Epoch 19/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.4482\n",
      "Epoch 20/20\n",
      "5278/5278 [==============================] - 28s 5ms/step - loss: 1.4213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ec83944190>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, labels, epochs=20, verbose=1, initial_epoch=0)\n",
    "# model = keras.models.load_model(\"model_200_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "aborted",
     "timestamp": 1641206485156,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "WeweULSNKrHc"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_lyrics(seed_txt, next_words_cnt , max_seq_len, model):\n",
    "  \n",
    "    for i in range(0, next_words_cnt):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_txt])[0]\n",
    "        padded_token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "        \n",
    "        prediction = np.argmax(model.predict(padded_token_list), axis=-1)\n",
    "\n",
    "        #print('prediction',prediction)\n",
    "        #tokenizer.word_index.items() --> rijecnik (rijec,index)\n",
    "        \n",
    "        \n",
    "        for (word, index) in tokenizer.word_index.items():\n",
    "            output_word = \"\"\n",
    "            if (prediction == index):\n",
    "                seed_txt += \" \" + word\n",
    "                break\n",
    "      \n",
    "        \n",
    "    return seed_txt.title()\n",
    "\n",
    "def generate_lyrics_endl_as_token(seed_txt,\n",
    "                                  lines_in_n_gram,\n",
    "                                  num_lines,\n",
    "                                  max_line_len,\n",
    "                                  max_seq_len,\n",
    "                                  model,\n",
    "                                  no_anaphora=False,\n",
    "                                  keep_seed=False):\n",
    "    # cleaning the seed text, adding endl and splitting into lines\n",
    "    st_list = (seed_txt.strip().replace('\\r', \"\").replace('\\n', \" endl\\n\")\n",
    "               + \" endl\").split('\\n')\n",
    "    # keep at most last lines_in_gram lines\n",
    "    st_list = st_list[max(0, len(st_list) - lines_in_n_gram):]\n",
    "    # seed text to tokens (list of tokens for each line)\n",
    "    token_list = tokenizer.texts_to_sequences(st_list)\n",
    "    if len(token_list) < lines_in_n_gram = 1:\n",
    "        \n",
    "    # how many lines do we need\n",
    "    lines_remaining = num_lines - (len(token_list) if keep_seed else 0)\n",
    "    # flatten all the tokens into single list\n",
    "    flat = [jt for it in token_list for jt in it]\n",
    "    endl_token = tokenizer.word_index[\"endl\"]\n",
    "    tokenized_song = []\n",
    "    curr_line = []\n",
    "    while lines_remaining:\n",
    "        # print(lines_remaining)\n",
    "        if len(curr_line) >= max_line_len:\n",
    "            prediction = endl_token\n",
    "        else:\n",
    "            padded_token_list = pad_sequences([flat], maxlen=max_seq_len-1, padding='pre')\n",
    "            predictions = model.predict(padded_token_list)\n",
    "            # print(type(predictions))\n",
    "            if (no_anaphora):\n",
    "                for prev_line in token_list:\n",
    "                    if (len(curr_line) < len(prev_line)\n",
    "                        and curr_line == prev_line[:len(curr_line)]):\n",
    "                        # print(curr_line)\n",
    "                        word_in_prev_line = prev_line[len(curr_line)]\n",
    "                        # print(word_in_prev_line)\n",
    "                        predictions[0][word_in_prev_line] = np.min(predictions, axis=-1)\n",
    "            prediction = np.argmax(predictions, axis=-1)[0]\n",
    "        curr_line.append(prediction)\n",
    "        flat.append(prediction)\n",
    "        if prediction == endl_token:\n",
    "            lines_remaining -= 1\n",
    "            if len(token_list) >= lines_in_n_gram: del token_list[0]\n",
    "            token_list.append(curr_line)\n",
    "            tokenized_song.append(curr_line[:-1])\n",
    "            curr_line = []\n",
    "            flat = [jt for it in token_list for jt in it]\n",
    "    # print(tokenized_song[1:])\n",
    "    print('\\n'.join(\n",
    "        tokenizer.sequences_to_texts(tokenized_song)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1641206485159,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "Z6luPyeqRMrn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cudnn model_200_10\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: cudnn model_200_10\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x000001EC83886C10> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cudnn model_200_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "aborted",
     "timestamp": 1641206485161,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "clRleXG5Ktgl",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i know i shouldn't\n",
      "but that's the way it is\n",
      "with the touch of your hand\n",
      "you can't fool why i do\n",
      "\n",
      "i don't know what i really am i\n",
      "if i saw you\n",
      "everyone's wrong but me\n",
      "you know it'll work alright\n",
      "lady be good do what you should\n",
      "\n",
      "'cause i found my heart in a crowd\n",
      "i prefer to live my side\n",
      "to feel my fisty cuffs\n",
      "you know you were the best he ever had oh oh oh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''unique = []\n",
    "for i in range(0,len(corpus)):\n",
    "    unique.append(corpus[i].split())'''\n",
    "\n",
    "'''\n",
    "for i in range(0,20):\n",
    "  random_list = unique[random.randint(0,len(unique)-1)]\n",
    "  seed = random_list[random.randint(0,len(random_list))]\n",
    "  print( generate_lyrics(seed,random.randint(3,15),max_seq_len, model ) )\n",
    "'''\n",
    "# duljina pjesme iz iste distribucije kao ostale\n",
    "# koristenje corpusa tu?\n",
    "num_lines = random.choice([it.count(\"\\n\") for it in all_lyrics])\n",
    "max_line_len = max([max([len(it.split()) for it in song.split(\"\\n\")]) for song in all_lyrics])\n",
    "\n",
    "\n",
    "seed_txt = \"\"\"Here's a little song I wrote\n",
    "You might want to sing it note for note\n",
    "Don't worry, be happy\"\"\"\n",
    "\n",
    "# seed_txt = \"\"\n",
    "\n",
    "generate_lyrics_endl_as_token(seed_txt,\n",
    "                              lines_in_n_gram=4,\n",
    "                              num_lines=num_lines,\n",
    "                              max_line_len=max_line_len,\n",
    "                              max_seq_len=max_seq_len,\n",
    "                              model=model,\n",
    "                              no_anaphora=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1641206485163,
     "user": {
      "displayName": "Eva Smuc",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14867067531466493907"
     },
     "user_tz": -60
    },
    "id": "0v9-8EZrX-zp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lyrics_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
